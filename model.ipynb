{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda, number of workers: 8\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision import disable_beta_transforms_warning\n",
    "disable_beta_transforms_warning()\n",
    "import torchvision.transforms.v2 as tf\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "workers = os.cpu_count()\n",
    "pu = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "assert pu == 'cuda', 'Connect to the GPU'\n",
    "\n",
    "print(f'using {pu}, number of workers: {workers}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_folder = '/content/gdrive/MyDrive'\n",
    "WORKSPACE = f'{home_folder}/Skin_Diseases_Detection'\n",
    "DATASET_FOLDER = f\"{home_folder}/skdi_dataset\"\n",
    "TRAINING_FOLDER = f'{DATASET_FOLDER}/train_dir'\n",
    "TESTING_FOLDER = f'{DATASET_FOLDER}/base_dir'\n",
    "CHECKPOINT_FOLDER = f'{WORKSPACE}/checkpoints'\n",
    "STATUS_FOLDER = f'{WORKSPACE}/status'\n",
    "PLOT_FOLDER = f'{WORKSPACE}/plots'\n",
    "PARAM_FOLDER = f'{WORKSPACE}/param'\n",
    "CONFIG_FOLDER = f'{WORKSPACE}/config'\n",
    "MODEL_FOLDER = f'{WORKSPACE}/model'\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, train_trans, test_trans, train_batch_size = 32, val_rat = 0.2):\n",
    "        self._ds = ImageFolder(TRAINING_FOLDER, train_trans)\n",
    "        self.test_ds = ImageFolder(TESTING_FOLDER, test_trans)\n",
    "        ds_size = len(self._ds)\n",
    "        val_size = int(val_rat*ds_size)\n",
    "        train_size = len(self._ds) - val_size\n",
    "\n",
    "        self.train_ds, self.val_ds = Subset(self._ds, range(train_size)), Subset(self._ds, range(train_size, ds_size))\n",
    "        self.train_dl = DataLoader(self.train_ds, batch_size=train_batch_size, shuffle=True, num_workers=workers)\n",
    "        self.test_dl = DataLoader(self.test_ds, batch_size=train_batch_size, shuffle=True, num_workers=workers)\n",
    "        self.val_dl = DataLoader(self.val_ds, batch_size=train_batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "def make_cnn(dataset: ImageFolder, hid_layers = [64, 64],\n",
    "            act_fn='relu', max_pool = None, pooling_after_layers = 2, dropout = 0.2, batch_norm=True,\n",
    "            conv_layers=[[32, 3, 1],\n",
    "                         [16, 3, 1]]):\n",
    "    \n",
    "    img = dataset.__getitem__(0)[0]\n",
    "    input_shape = img.shape\n",
    "    num_of_classes = len(dataset.classes)\n",
    "    layers = []\n",
    "    activation_fun = {'relu': nn.ReLU(), 'softplus':nn.Softplus(), 'tanh':nn.Tanh(), 'elu': nn.ELU()}\n",
    "\n",
    "    assert pooling_after_layers < len(conv_layers), 'exceeding the number conv layers..'\n",
    "\n",
    "    in_chann, inp_h, inp_w = input_shape\n",
    "    for ind, conv in enumerate(conv_layers):\n",
    "        out_chann, filter_size, stride = conv\n",
    "        layers.append(nn.Conv2d(in_chann, out_chann, filter_size, stride))\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_chann))\n",
    "        layers.append(activation_fun[act_fn])\n",
    "\n",
    "        out_h = (inp_h - filter_size)//stride + 1\n",
    "        out_w = (inp_w - filter_size)//stride + 1\n",
    "        inp_h = out_h\n",
    "        inp_w = out_w\n",
    "\n",
    "        if max_pool is not None and ((ind+1) % pooling_after_layers == 0 or ind == (len(conv_layers) - 1)):\n",
    "            layers.append(nn.MaxPool2d(max_pool[0], max_pool[1]))\n",
    "            out_h = (inp_h - max_pool[0])//max_pool[1] + 1\n",
    "            out_w = (inp_w - max_pool[0])//max_pool[1] + 1\n",
    "            inp_h = out_h\n",
    "            inp_w = out_w\n",
    "        in_chann = out_chann\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "    layers.append(nn.Linear(inp_h*inp_w*in_chann, hid_layers[0]))\n",
    "    layers.append(activation_fun[act_fn])\n",
    "    if len(hid_layers) > 1:\n",
    "        dim_pairs = zip(hid_layers[:-1], hid_layers[1:])\n",
    "        for in_dim, out_dim in list(dim_pairs):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if dropout is not None:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(activation_fun[act_fn])\n",
    "\n",
    "    layers.append(nn.Linear(hid_layers[-1], num_of_classes))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def convert(**kwargs):\n",
    "    return kwargs\n",
    "\n",
    "class Utils:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_file = ''\n",
    "        self.plot_file = ''\n",
    "        self.min_loss = 0\n",
    "\n",
    "    def read_file(self, path):\n",
    "        file = open(path, 'r')\n",
    "        file.seek(0)\n",
    "        info = file.readline()\n",
    "        file.close()\n",
    "        return info\n",
    "\n",
    "    def write_file(self, path, content):\n",
    "        mode = 'w'\n",
    "        if path == self.plot_file:\n",
    "            mode = '+a'\n",
    "        file = open(path, mode=mode)\n",
    "        file.write(content)\n",
    "        file.close()\n",
    "\n",
    "    def create_file(self, path):\n",
    "        with open(path, 'w') as file:\n",
    "            pass\n",
    "        file.close()\n",
    "\n",
    "    def create_checkpoint_file(self, num):\n",
    "        path = f'{CHECKPOINT_FOLDER}/checkpoint_{num}.pth'\n",
    "        file = open(path, 'w')\n",
    "        file.close()\n",
    "        return path\n",
    "    \n",
    "    def save_config(self, args: dict):\n",
    "        if not os.path.exists(self.config_file):\n",
    "            self.create_file(self.config_file)\n",
    "        with open(self.config_file, 'w') as file:\n",
    "            yaml.safe_dump(args, file)\n",
    "        file.close()\n",
    "\n",
    "    def check_status_file(self):\n",
    "        if not os.path.exists(self.status_file):\n",
    "            self.create_file(self.status_file)\n",
    "        checkpath = self.read_file(self.status_file)\n",
    "        epoch = 0\n",
    "        if checkpath != '':\n",
    "            epoch = self.load_checkpoint(checkpath)\n",
    "            file = open(self.plot_file, 'r')\n",
    "            lines = file.readlines()\n",
    "            file = open(self.plot_file, 'w')\n",
    "            file.writelines(lines[:epoch+1])\n",
    "            file.close()\n",
    "        else:\n",
    "            file = open(self.plot_file, 'w')\n",
    "            file.close()\n",
    "            self.write_file(self.plot_file,'Train_loss,Train_acc,Valid_loss,Valid_acc\\n')\n",
    "            self.model.train()\n",
    "        return epoch\n",
    "\n",
    "    def write_plot_data(self, data:list):\n",
    "        str_data = ','.join(map(str, data))\n",
    "        self.write_file(self.plot_file, f'{str_data}\\n')\n",
    "\n",
    "    def save_checkpoint(self, epoch, checkpath):\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optim_state_dict': self.optim.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        file = open(self.status_file, 'w')\n",
    "        file.write(checkpath)\n",
    "        file.close()\n",
    "        torch.save(checkpoint, checkpath)\n",
    "        print('checkpoint saved..')\n",
    "    \n",
    "    def load_checkpoint(self, checkpath):\n",
    "        print('loading checkpoint..')\n",
    "        checkpoint = torch.load(checkpath)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
    "        self.model.train()\n",
    "        print('checkpoint loaded...')\n",
    "        return checkpoint['epoch']\n",
    "    \n",
    "    def save_check_interval(self, epoch, interval=50):\n",
    "        if not(epoch % interval) and epoch > 0:\n",
    "            checkpath = self.create_checkpoint_file(epoch)\n",
    "            self.save_checkpoint(epoch, checkpath)\n",
    "    \n",
    "    def load_model(self):\n",
    "        print('loading model...')\n",
    "        self.model.load_state_dict(torch.load(self.model_file))\n",
    "        self.model.eval()\n",
    "        print('model loaded...')\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), self.model_file)\n",
    "        print('model saved...')\n",
    "\n",
    "    def save_best_model(self, param, acc_param=True):\n",
    "        if acc_param:\n",
    "           param_ = max(param, self.param)\n",
    "        else:\n",
    "            param_ = min(param, self.param)\n",
    "        self.param = param_\n",
    "        self.write_file(self.param_file, f'{param_}')\n",
    "        self.save_model()\n",
    "\n",
    "    def check_param_file(self):\n",
    "        if os.path.exists(self.param_file):\n",
    "            param = float(self.read_file(self.param_file))\n",
    "        else:\n",
    "            self.create_file(self.param_file)\n",
    "            param = -1000.0\n",
    "            self.write_file(self.param_file, f'{param}')\n",
    "        return param\n",
    "    \n",
    "\n",
    "class skdi_detector(Utils):\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "        self.dataset = Dataset(train_trans=params.train_trans, test_trans=params.test_trans,\n",
    "                               train_batch_size=params.batch_size, val_rat=params.val_rat)\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.name = params.name\n",
    "        self.model_file = f'{MODEL_FOLDER}/{self.name}_model.pth'\n",
    "        self.status_file = f'{STATUS_FOLDER}/{self.name}_status.txt'\n",
    "        self.plot_file = f'{PLOT_FOLDER}/{self.name}_plot.txt'\n",
    "        self.param_file = f'{PARAM_FOLDER}/{self.name}_param.txt'\n",
    "        self.config_file = f'{CONFIG_FOLDER}/{self.name}_config.yaml'\n",
    "        self.param = self.check_param_file()\n",
    "        self.metric_param = params.metric_param\n",
    "        self.clip_grad = params.clip_grad\n",
    "        self.acc_param = False\n",
    "        if self.metric_param in ['train_acc', 'val_acc']:\n",
    "            self.acc_param = True\n",
    "\n",
    "    def train_step(self):\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for _, (x, y) in enumerate(self.dataset.train_dl):\n",
    "            x, y = x.to(pu), y.to(pu)\n",
    "            y_pred_logits = self.model(x)\n",
    "\n",
    "            loss = self.loss_fn(y_pred_logits, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n",
    "            self.optim.step()\n",
    "\n",
    "            y_pred = torch.argmax(torch.softmax(y_pred_logits, dim=-1), dim=-1)\n",
    "            train_acc += (y_pred == y).sum().item()/len(y)\n",
    "        \n",
    "        train_loss /= len(self.dataset.train_dl)\n",
    "        train_acc /= len(self.dataset.train_dl)\n",
    "\n",
    "        return train_loss, train_acc\n",
    "    \n",
    "    def validate_step(self):\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for _, (x, y) in enumerate(self.dataset.val_dl):\n",
    "                x, y = x.to(pu), y.to(pu)\n",
    "                y_pred_logits = self.model(x)\n",
    "\n",
    "                loss = self.loss_fn(y_pred_logits, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                y_pred = torch.argmax(torch.softmax(y_pred_logits, dim=-1), dim=-1)\n",
    "                val_acc += (y_pred == y).sum().item()/len(y)\n",
    "        \n",
    "        val_loss /= len(self.dataset.val_dl)\n",
    "        val_acc /= len(self.dataset.val_dl)\n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.model = make_cnn(dataset=self.dataset._ds, hid_layers=self.params.hid_layers, act_fn=self.params.act_fn,\n",
    "                              max_pool=self.params.max_pool, pooling_after_layers=self.params.pool_after_layers,\n",
    "                              batch_norm=self.params.batch_norm, conv_layers=self.params.conv_layers, dropout=self.params.dropout).to(pu)\n",
    "        \n",
    "        self.optim = Adam(self.model.parameters(), lr=self.params.lr, eps=1e-6, weight_decay=1e-5)\n",
    "        print(f'Model: {self.model}')\n",
    "        print(f'Number of classes: {self.dataset._ds.classes}')\n",
    "        print(f'Input image size: {self.dataset._ds.__getitem__(0)[0][0].shape}')\n",
    "        \n",
    "    def plot_images(self):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "        for i in range(3):\n",
    "            ind = random.randint(0, len(self.dataset.train_ds)-1)\n",
    "            img = self.dataset.train_ds.__getitem__(ind)[0]\n",
    "            img = img.numpy()\n",
    "            img = img.transpose((1, 2, 0)) if img.shape[0] == 3 else img\n",
    "            axes.flat[i].imshow(img)\n",
    "            axes.flat[i].axis('off')\n",
    "\n",
    "    def train(self):\n",
    "        epochs = self.params.epochs\n",
    "        epoch = 0\n",
    "        epoch = self.check_status_file()\n",
    "        print(f'training for {epochs} epochs....')\n",
    "        for ep in tqdm(range(epoch, epochs+1)):\n",
    "            train_loss, train_acc = self.train_step()\n",
    "            val_loss, val_acc = self.validate_step()\n",
    "\n",
    "            metric_param = {'train_loss': train_loss, 'train_acc': train_acc,\n",
    "                            'val_loss': val_loss, 'val_acc': val_acc}\n",
    "            \n",
    "            print(f'epochs: {ep}\\t{train_loss = :.4f}\\t{train_acc = :.4f}\\t{val_loss = :.4f}\\t{val_acc = :.4f}')\n",
    "            self.write_plot_data([train_loss, train_acc, val_loss, val_acc])\n",
    "            self.save_check_interval(epoch=ep, interval=1)\n",
    "            self.save_best_model(acc_param=self.acc_param, param=metric_param[self.metric_param])\n",
    "        \n",
    "        print('Finished Training....')\n",
    "    \n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.name = 'model'\n",
    "        self.conv_layers = [[128, 3, 1],\n",
    "                            [128, 3, 1],\n",
    "                            [64, 3, 1],\n",
    "                            [64, 3, 1],\n",
    "                            [32, 3, 1],\n",
    "                            [32, 3, 1],\n",
    "                            [16, 3, 1],\n",
    "                            [16, 3, 1]]\n",
    "        self.hid_layers = [1024, 512, 256]\n",
    "        self.max_pool = [2, 2]\n",
    "        self.pool_after_layers = 2\n",
    "        self.act_fn = 'relu'\n",
    "        self.lr = 1e-5\n",
    "        self.epochs = 100\n",
    "        self.clip_grad = 0.3\n",
    "        self.metric_param = 'val_acc'\n",
    "        self.batch_size = 16\n",
    "        self.batch_norm = True\n",
    "        self.dropout = 0.2\n",
    "        self.test_trans = tf.Compose([\n",
    "            tf.Resize(size = (256,256)),\n",
    "            tf.ToTensor()\n",
    "        ])\n",
    "        self.train_trans = tf.Compose([\n",
    "            tf.Resize(size=(256, 256)),\n",
    "            tf.RandomHorizontalFlip(p=0.8),\n",
    "            tf.RandomVerticalFlip(),\n",
    "            tf.ToTensor()\n",
    "        ])\n",
    "        self.val_rat = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()\n",
    "\n",
    "agent = skdi_detector(params)\n",
    "\n",
    "agent.create_model()\n",
    "print(f'training dataset size: {len(agent.dataset.train_ds)}')\n",
    "print(f'validation dataset size: {len(agent.dataset.val_ds)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
